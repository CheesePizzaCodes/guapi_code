Index: ml/visualize.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import List\r\n\r\nimport matplotlib.pyplot as plt\r\nimport plotly.graph_objects as go\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.preprocessing import LabelEncoder\r\n\r\nimport file_io\r\nimport formatting\r\n\r\nle = LabelEncoder()\r\n\r\n\r\ndef scatter_3d(df: pd.DataFrame, col_names: List[str], classes: str = None):\r\n    f: plt.Figure = plt.figure()\r\n    a: plt.Axes = f.add_subplot(projection='3d')\r\n    label_names = df[classes].values\r\n    label_names_unique = np.unique(label_names)\r\n    le.fit(list(set(label_names)))\r\n    encoding = le.transform(label_names)\r\n    df_copy = df.copy()\r\n    for label in label_names_unique:\r\n        df = df.loc[df[classes] == label]  # select correct class\r\n        df = df[col_names]\r\n        # data_slice = data_slice[col_names]\r\n        a.scatter(*df.values.T.astype('float'),  label=label)\r\n        df = df_copy\r\n\r\n    a.set_xlabel(cols[0])\r\n    a.set_ylabel(cols[1])\r\n    a.set_zlabel(cols[2])\r\n    box = a.get_position()\r\n    a.set_position([box.x0, box.y0, box.width * 0.8, box.height])\r\n\r\n    # Put a legend to the right of the current axis\r\n    a.legend(loc='center left', bbox_to_anchor=(1, 0.5))\r\n\r\n    return f, a\r\n\r\n\r\nif __name__ == '__main__':\r\n    data = file_io.load_data('./out_data/finalisimo.json')\r\n    data = formatting.format_data(data)\r\n    cols = ['LOA [m]', 'S.A./Disp.', 'Beam [m]']\r\n    fig, a = scatter_3d(data, cols, classes='Hull Type')\r\n    a.legend()\r\n    plt.show()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ml/visualize.py b/ml/visualize.py
--- a/ml/visualize.py	(revision bb7a829fa048b5fd97d8b8683be4eb98045fdde8)
+++ b/ml/visualize.py	(date 1681349185481)
@@ -1,9 +1,10 @@
 from typing import List
 
-import matplotlib.pyplot as plt
-import plotly.graph_objects as go
+# import matplotlib.pyplot as plt
+# import plotly.graph_objects as go
+import plotly.express as px
 import pandas as pd
-import numpy as np
+# import numpy as np
 from sklearn.preprocessing import LabelEncoder
 
 import file_io
@@ -11,38 +12,64 @@
 
 le = LabelEncoder()
 
+#
+# def scatter_3d(df: pd.DataFrame, col_names: List[str], classes: str = None):
+#     """
+#     Obsolete.
+#     :param df:
+#     :param col_names:
+#     :param classes:
+#     :return:
+#     """
+#     f: plt.Figure = plt.figure()
+#     a: plt.Axes = f.add_subplot(projection='3d')
+#     label_names = df[classes].values
+#     label_names_unique = np.unique(label_names)
+#     le.fit(list(set(label_names)))
+#     encoding = le.transform(label_names)
+#     df_copy = df.copy()
+#     for label in label_names_unique:
+#         df = df.loc[df[classes] == label]  # select correct class
+#         df = df[col_names]
+#         # data_slice = data_slice[col_names]
+#         a.scatter(*df.values.T.astype('float'), label=label)
+#         df = df_copy
+#
+#     a.set_xlabel(cols[0])
+#     a.set_ylabel(cols[1])
+#     a.set_zlabel(cols[2])
+#     box = a.get_position()
+#     a.set_position([box.x0, box.y0, box.width * 0.8, box.height])
+#
+#     # Put a legend to the right of the current axis
+#     a.legend(loc='center left', bbox_to_anchor=(1, 0.5))
+#
+#     return f, a
 
-def scatter_3d(df: pd.DataFrame, col_names: List[str], classes: str = None):
-    f: plt.Figure = plt.figure()
-    a: plt.Axes = f.add_subplot(projection='3d')
-    label_names = df[classes].values
-    label_names_unique = np.unique(label_names)
-    le.fit(list(set(label_names)))
-    encoding = le.transform(label_names)
-    df_copy = df.copy()
-    for label in label_names_unique:
-        df = df.loc[df[classes] == label]  # select correct class
-        df = df[col_names]
-        # data_slice = data_slice[col_names]
-        a.scatter(*df.values.T.astype('float'),  label=label)
-        df = df_copy
 
-    a.set_xlabel(cols[0])
-    a.set_ylabel(cols[1])
-    a.set_zlabel(cols[2])
-    box = a.get_position()
-    a.set_position([box.x0, box.y0, box.width * 0.8, box.height])
+def scatter_3d_p(df: pd.DataFrame, col_names: List[str], classes: str = None):
+    fig = px.scatter_3d(df,
+                        x=col_names[0], y=col_names[1], z=col_names[2],
+                        color=classes,
+                        )
+    fig.update_traces(marker_size=2)
+    return fig
 
-    # Put a legend to the right of the current axis
-    a.legend(loc='center left', bbox_to_anchor=(1, 0.5))
 
-    return f, a
+def show(fig):
+    import io
+    import plotly.io as pio
+    from PIL import Image
+    buf = io.BytesIO()
+    pio.write_image(fig, buf)
+    img = Image.open(buf)
+    img.show()
 
 
 if __name__ == '__main__':
     data = file_io.load_data('./out_data/finalisimo.json')
     data = formatting.format_data(data)
     cols = ['LOA [m]', 'S.A./Disp.', 'Beam [m]']
-    fig, a = scatter_3d(data, cols, classes='Hull Type')
-    a.legend()
-    plt.show()
+    fig2 = scatter_3d_p(data, cols, 'Hull Type')
+    fig2.show()
+    ...
Index: ml/file_io.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import json\r\nfrom typing import List, Dict, Tuple\r\n\r\nimport numpy as np\r\nfrom gensim.scripts.glove2word2vec import glove2word2vec\r\n\r\n\r\ndef write_data_to_file(data: List[Dict[str, str]], filename) -> str:\r\n    \"\"\"\r\n\r\n    :param filename:\r\n    :param data:\r\n    :return: File name\r\n    \"\"\"\r\n    # # Write\r\n    with open(f'./out_data/{filename}.json', 'w') as file:\r\n        # writing the out_data into the file\r\n        json.dump(data, file, indent=4)\r\n\r\n    print(f\"Writing to {file.name}\")\r\n\r\n    return file.name\r\n\r\n\r\ndef load_data(json_file_path: str) -> List[Dict[str, str]]:\r\n    \"\"\"\r\n    Loads the out_data in the raw format\r\n    :param json_file_path:\r\n    :return:\r\n    \"\"\"\r\n    with open(json_file_path) as f:\r\n        data = json.load(f)\r\n    return data\r\n\r\n\r\ndef load_embedding(file_path: str = './data/glove/glove.840B.300d.txt', verbose: bool=False) -> Dict[str, np.ndarray]:\r\n    \"\"\"\r\n    TODO does this really belong here??\r\n    Loads the GloVe pretrained embeddings as a dictionary\r\n    https://nlp.stanford.edu/projects/glove/\r\n    :param verbose: print full traceback for failed data collection\r\n    :param file_path: path of the GloVe file\r\n    :return: Dictionary mapping a word to a 300D vector as a ndarray\r\n    \"\"\"\r\n    embeddings_index = {}  # initialize dictionary\r\n    with open(file_path, encoding='utf-8') as f:\r\n        c = 1\r\n        for line in f:\r\n            values = line.split()\r\n            word = values[0]\r\n            # print(c)\r\n            if c == 52342:\r\n                pass\r\n            try:\r\n                coefs = np.asarray(values[1:], dtype='float32')\r\n            except ValueError as e:  # some entries contain literals that cannot be converted to float\r\n                print(f'Word number {c} failed. Word followed by first values:  {values[:10]}')\r\n                if verbose:\r\n                    print(e)\r\n            embeddings_index[word] = coefs\r\n            c += 1\r\n    return embeddings_index\r\n\r\n\r\ndef save_embedding_w2v(file_path: str = './data/glove/glove.840B.300d.txt',\r\n                       out_file: str = './out_data/glove/glove.840B.300d.txt') -> Tuple[int, int]:\r\n    \"\"\"\r\n    Uses external tool to save embedding in a specific format\r\n    :param file_path:\r\n    :param out_file:\r\n    :return:\r\n    \"\"\"\r\n    return glove2word2vec(file_path, out_file)
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ml/file_io.py b/ml/file_io.py
--- a/ml/file_io.py	(revision bb7a829fa048b5fd97d8b8683be4eb98045fdde8)
+++ b/ml/file_io.py	(date 1681349673867)
@@ -7,7 +7,7 @@
 
 def write_data_to_file(data: List[Dict[str, str]], filename) -> str:
     """
-
+    Writes scraping output data to json file
     :param filename:
     :param data:
     :return: File name
@@ -24,7 +24,7 @@
 
 def load_data(json_file_path: str) -> List[Dict[str, str]]:
     """
-    Loads the out_data in the raw format
+    Loads the scraping out_data in the raw format
     :param json_file_path:
     :return:
     """
Index: ml/formatting.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import List, Dict, Union\r\n\r\nimport re\r\n\r\nimport pandas as pd\r\n\r\n\r\ndef remove_colon_from_titles(data: List[Dict]) -> List[Dict]:\r\n    return [{k.replace(':', ''): v for (k, v) in dictio.items()} for dictio in data]\r\n\r\n\r\ndef add_units_to_titles(data: List[Dict]) -> List[Dict]:\r\n    return [{f'{k}{extract_units(v)}': v for (k, v) in dictio.items()} for dictio in data]\r\n\r\n\r\ndef convert_values_to_numeric(data: List[Dict]) -> List[Dict]:\r\n    return [{k: extract_value(v) for (k, v) in dictio.items()} for dictio in data]\r\n\r\n\r\ndef convert_decimal_separator(data: List[Dict]) -> List[Dict]:\r\n    return [{k: v.replace(',', '.') for (k, v) in dictio.items()} for dictio in data]\r\n\r\n\r\ndef has_numbers(_string):\r\n    return bool(re.search(r'\\d', _string))\r\n\r\n\r\ndef ends_with_any_of(target_string: str, string_list: List[str]) -> bool:\r\n    return any(target_string.endswith(ending) for ending in string_list)\r\n\r\n\r\nUNITS = [' L', ' mÂ²', ' kg', ' m', ' kn', ' pounds/inch']\r\n\r\n\r\ndef extract_value(raw_string: str) -> Union[float, str]:\r\n    \"\"\"\r\n    Extracts either the numerical value and converts it to a float or the string value\r\n    :param raw_string:\r\n    :return:\r\n    \"\"\"\r\n    # first case: data already numeric\r\n    if raw_string.isnumeric():\r\n        return float(raw_string)\r\n    # second case: check for numbers and ending in units\r\n    if has_numbers(raw_string) & ends_with_any_of(raw_string, UNITS):\r\n        match = re.match(r'(\\d*\\.*\\d+)', raw_string)  # extract numbers\r\n        if match is None:  # insurance agains false positives such as 'MD 22 L'\r\n            return raw_string\r\n        return float(match.group(1))\r\n    # third case: none, string type\r\n    return raw_string\r\n\r\n\r\ndef extract_units(raw_string: str) -> str:\r\n    \"\"\"\r\n    Extracts units from the string\r\n    :param raw_string:\r\n    :return:\r\n    \"\"\"\r\n    units = list(filter(lambda u: u in raw_string, UNITS))\r\n    if not units:\r\n        return ''\r\n    if 'mÂ²' in units[0]:\r\n        print('2')\r\n    return f' [{units[0].strip()}]'\r\n\r\n\r\ndef format_data(data: List[Dict[str, str]]) -> pd.DataFrame:\r\n    \"\"\"\r\n    Transforms the raw string out_data to the correct numeric type\r\n    :param data:\r\n    :return:\r\n    \"\"\"\r\n    data = remove_colon_from_titles(data)\r\n    data = convert_decimal_separator(data)\r\n    data = add_units_to_titles(data)\r\n    data = convert_values_to_numeric(data)\r\n    return pd.DataFrame(data)\r\n\r\n\r\ndef test_1():\r\n    from ml.file_io import load_data\r\n\r\n    data = load_data('./out_data/finalisimo.json')\r\n    df = pd.DataFrame(data)\r\n    data2 = format_data(data)\r\n\r\n    df = pd.DataFrame(data2)\r\n    ...\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_1()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ml/formatting.py b/ml/formatting.py
--- a/ml/formatting.py	(revision bb7a829fa048b5fd97d8b8683be4eb98045fdde8)
+++ b/ml/formatting.py	(date 1681349287390)
@@ -2,9 +2,10 @@
 
 import re
 
+import numpy as np
 import pandas as pd
 
-
+# ## ---------------- UTILS ---------------- # ##
 def remove_colon_from_titles(data: List[Dict]) -> List[Dict]:
     return [{k.replace(':', ''): v for (k, v) in dictio.items()} for dictio in data]
 
@@ -31,6 +32,7 @@
 
 UNITS = [' L', ' mÂ²', ' kg', ' m', ' kn', ' pounds/inch']
 
+# ## ---------------- FUNCS ---------------- # ##
 
 def extract_value(raw_string: str) -> Union[float, str]:
     """
@@ -91,3 +93,7 @@
 
 if __name__ == '__main__':
     test_1()
+
+
+def replace_strings(_words: Union[List[str], np.ndarray], to_replace: str, replace_with: str) -> np.ndarray:
+    return np.asarray([word.replace(to_replace, replace_with) for word in _words])
Index: ml/language_processing.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\r\nModule for processing the scientific language.\r\nMain objective is clustering the terms to reduce unique categories of e.g. manufacturing process, etc.\r\n\"\"\"\r\nfrom typing import List, Union\r\nfrom distance import levenshtein\r\n\r\nimport numpy as np\r\nfrom sklearn.cluster import AffinityPropagation\r\nimport pandas as pd\r\nfrom scipy.spatial.distance import pdist\r\nfrom gensim.models import KeyedVectors\r\n\r\n\r\nimport file_io\r\nimport formatting\r\nfrom ml.file_io import load_embedding\r\n\r\n\r\ndef replace_strings(_words: Union[List[str], np.ndarray], to_replace: str, replace_with: str) -> np.ndarray:\r\n    return np.asarray([word.replace(to_replace, replace_with) for word in _words])\r\n\r\ndef remove_periods(words_list) -> np.ndarray:\r\n    \"\"\"\r\n    Remove points from the list of terms for avoiding problems\r\n    :param words_list:\r\n    :return:\r\n    \"\"\"\r\n    [word.replace('.', '') for word in words_list]\r\n    return np.array([word.replace('.', '') for word in words_list])\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    data = file_io.load_data('./out_data/finalisimo.json')\r\n    data = formatting.format_data(data)  # dataframe\r\n\r\n    attrs = ['Hull Type', 'Rigging Type', 'Construction']  # TODO remove [-] and keep this list elsewhere\r\n    i = 1\r\n\r\n    words = data[attrs[i]].values  # select column\r\n    words = np.unique(np.asarray(words, dtype='str'))  # remove duplicates\r\n    words = replace_strings(words, 'Frac.', 'Fractional')\r\n    _n = words.size\r\n\r\n    lev_similarity = -1 * pdist(words.reshape(-1, 1), levenshtein)\r\n\r\n    mat = np.zeros((_n, _n))\r\n    idx = np.triu_indices(_n, k=1)\r\n    mat[idx] = lev_similarity\r\n\r\n    affprop = AffinityPropagation(affinity=\"precomputed\", damping=0.7, verbose=True, max_iter=50000)\r\n    affprop.fit(mat)\r\n\r\n    output = []\r\n    for cluster_id in np.unique(affprop.labels_):\r\n        exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\r\n        cluster = np.unique(words[np.nonzero(affprop.labels_ == cluster_id)])\r\n        output.append(cluster)\r\n        cluster_str = \", \".join(cluster)\r\n        print(\" - *%s:* %s\" % (exemplar, cluster_str))\r\n    print(f'{np.unique(affprop.labels_).size} Clusters produced')\r\n    print(output)\r\n\r\n    emb = load_embedding()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ml/language_processing.py b/ml/language_processing.py
--- a/ml/language_processing.py	(revision bb7a829fa048b5fd97d8b8683be4eb98045fdde8)
+++ b/ml/language_processing.py	(date 1681349673852)
@@ -2,56 +2,26 @@
 Module for processing the scientific language.
 Main objective is clustering the terms to reduce unique categories of e.g. manufacturing process, etc.
 """
-from typing import List, Union
 from distance import levenshtein
 
 import numpy as np
 from sklearn.cluster import AffinityPropagation
-import pandas as pd
 from scipy.spatial.distance import pdist
-from gensim.models import KeyedVectors
-
 
 import file_io
 import formatting
 from ml.file_io import load_embedding
-
-
-def replace_strings(_words: Union[List[str], np.ndarray], to_replace: str, replace_with: str) -> np.ndarray:
-    return np.asarray([word.replace(to_replace, replace_with) for word in _words])
-
-def remove_periods(words_list) -> np.ndarray:
-    """
-    Remove points from the list of terms for avoiding problems
-    :param words_list:
-    :return:
-    """
-    [word.replace('.', '') for word in words_list]
-    return np.array([word.replace('.', '') for word in words_list])
-
+from ml.formatting import replace_strings
 
 
-if __name__ == '__main__':
-    data = file_io.load_data('./out_data/finalisimo.json')
-    data = formatting.format_data(data)  # dataframe
-
-    attrs = ['Hull Type', 'Rigging Type', 'Construction']  # TODO remove [-] and keep this list elsewhere
-    i = 1
-
-    words = data[attrs[i]].values  # select column
-    words = np.unique(np.asarray(words, dtype='str'))  # remove duplicates
-    words = replace_strings(words, 'Frac.', 'Fractional')
+def affprop_cluster():
     _n = words.size
-
     lev_similarity = -1 * pdist(words.reshape(-1, 1), levenshtein)
-
     mat = np.zeros((_n, _n))
     idx = np.triu_indices(_n, k=1)
     mat[idx] = lev_similarity
-
     affprop = AffinityPropagation(affinity="precomputed", damping=0.7, verbose=True, max_iter=50000)
     affprop.fit(mat)
-
     output = []
     for cluster_id in np.unique(affprop.labels_):
         exemplar = words[affprop.cluster_centers_indices_[cluster_id]]
@@ -62,4 +32,20 @@
     print(f'{np.unique(affprop.labels_).size} Clusters produced')
     print(output)
 
-    emb = load_embedding()
+    return output
+
+
+
+
+if __name__ == '__main__':
+    data = file_io.load_data('./out_data/finalisimo.json')
+    data = formatting.format_data(data)  # dataframe
+    # Categorical attributes
+    attrs = ['Hull Type', 'Rigging Type', 'Construction']  # TODO remove [-] and keep this list elsewhere
+    i = 1
+
+    words = data[attrs[i]].values  # select column
+    words = np.unique(np.asarray(words, dtype='str'))  # remove duplicates and change dtype to str
+    words = replace_strings(words, 'Frac.', 'Fractional')
+
+    affprop_cluster()
